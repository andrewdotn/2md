<!-- Copyright © 2020 Andrew Neitsch all rights reserved. -->

import { PauseableImage } from './pauseable-image';
import {
  DemoCharCount,
  DemoContent,
  DemoDom,
  DemoFlow,
  DemoImage,
  DemoHtmlEditor,
  DemoIntermediate,
  DemoMarkdown,
  DemoOutputBlocks,
  DemoUntransformedIntermediate,
} from "./demo-flow.tsx";

<div className="container-fluid px-0">
<div className="jumbotron d-md-flex flex-column align-items-center">

# Compiling HTML to Markdown with TypeScript

## How [2md][] works
[2md]: https://2md.ca

</div>
</div>

<div className="container">

[*2020-04-04: I’d been hoping to give this as a talk at a local developer
conference today, but since that was cancelled, I’ve written this article
instead.*]

2md is a tool I built so that I could copy-and-paste from browsers into
plain-text files, while preserving a lot of the formatting by automatically
converting the HTML to markdown.

It works on mac, linux, and windows, reading HTML off the clipboard from
the command line. You can see source code and install instructions [on
GitHub][2md-github].

[2md-github]: https://github.com/andrewdotn/2md

<figure className="figure">
  <PauseableImage src="/doc/demo.gif" className="figure-img img-fluid rounded"/>
  <figcaption className="figure-caption">
    Copying from the browser and getting markdown in the macOS terminal.<br/>
    Click or tap to pause animation.
  </figcaption>
</figure>

Since I wrote it in TypeScript, it works in the browser too, so you can
even try it out right here on this page.

<DemoFlow>

<DemoContent/>
<div className="clearfix"/>

2md turns the markup for that into this markdown:

<DemoMarkdown/>
<div className="clearfix"/>

# How 2md works

In this article, I’m going to walk through how this code works in case
you’re just curious or want to be able to build something like this
yourself. At some point this would definitely have been beyond my
capabilities, and now it’s relatively straightforward for me. But there’s
no single place I know of that can help people learn how to do this. I’m
hoping to fill that gap a bit.

I’m also going to talk about why I think TypeScript is great.

And I’m going to cover some pretty gritty low-level programming details; a
lot of learning is the accumulation of low-level tricks over many years. I
love reading about those tricks in case they come in handy some day, and
enjoy sharing them too.

<!-- wrapper is used to target styling for toc headings -->
<div className="toc">

### Table of Contents

<!--
  -- the div end has to come after the next header because remark-toc
  -- replaces everything in this section with the table of contents
  -->

## Motivation
</div>

For many years I’ve kept a terminal open in one corner of the
screen where I make notes in a text editor while I work. It’s sort of like
talking to myself while I work, but it doesn’t seem weird because it’s
silent, and I end up with a searchable, timestamped record of what I was
thinking while I was working on something: what issues I was running into
when programming, the options I considered, sources I consulted, and the
resolution I ultimately arrived at.

When my work involves bits of research, I often copy and paste relevant
snippets from web sources such as [O’Reilly books][], Stack Overflow,
documentation, source code, and so on. In the moment it’s really handy to
be able to glance at the corner of the screen to see the most pertinent
documentation excerpts. Then later, I often run into an error message that
seems familiar, and am able to search upwards in my notes to find the
context around where I ran into that error message before, and the relevant
snippet that explains how to fix it.

[O’Reilly books]: https://learning.oreilly.com/

As I’ve gotten used to markdown over the years, my text notes have taken on
a markdown-ish flavour. You have to know how to use markdown to contribute
on Stack Overflow or write pull request descriptions on GitHub. And once
you know it, it becomes very convenient for general writing about
programming: it’s faster and easier to type and read and diff than raw
HTML. So I naturally started using it not only for writing things like this
article, but also in my notes. My notes aren’t all markdown because they go
back many many years and it’s still mostly me talking to myself, but I try
to use it where I can easily do so.

When pasting from the web to a `.txt` file, I found that I’d started
manually massaging what I’d pasted to make it markdown-ish.

  - Adding `>` characters at the start to clearly distinguish quotes from
    my own thoughts.
  - Wrapping the text to fit in 80 columns.
  - Separating out bullet points into little text lists.
  - Preserving *italics* where important.
  - Inserting hyperlinks.

After doing this a bunch of times, I started to think, “Surely I can
automate this.” And I was probably doing this a few times a day, just a
moment or two of rearranging a handful of lines of text in my text editor,
but getting those regular reminders saying, “I can automate this.”

Neal Ford in [The Productive Programmer][], discussing automation,
describes that when weighing whether to do a task via brute force or to
spend time and effort automating it, while automation often pays off
time-wise,

[The Productive Programmer]: https://learning.oreilly.com/library/view/the-productive-programmer/9780596519780/ch04.html#I_sect13_d1e5404

> … that’s not the important point. Performing simple, repetitive tasks
> by hand makes you dumber, and it steals part of your concentration, which
> is your most productive asset.
>
> Figuring out a clever way to automate the task makes you smarter because
> you learn something along the way.

One day I let these constant reminders get through to me, started
automating it, and am really glad I did.

### Related work

I built this for myself as a fun challenge, never pausing to consider
whether it already existed until after I’d coded it up myself from scratch.

It wasn’t until the initial version was working pretty well and I was
updating the project description on the GitHub page that I saw that were
was already an [html-to-markdown tag][] on GitHub with lots of projects,
many already written in JavaScript.

[html-to-markdown tag]: https://github.com/topics/html-to-markdown

The [turndown][] project is the big one; if you want a much more mature
html-to-markdown tool, you might want to take a look at that.

[turndown]: https://github.com/domchristie/turndown

And then many months later I was mortified to find that
[github.com/phodal/2md][] and [github.com/zshipu/2md][] had already existed
for three years, with an interactive web version at
[phodal.github.io/2md/][]! Same name and everything. Oops. When I saw that
the `2md` package name was free on [npm][], I assumed that the name `2md`
was totally original. And by the time I found out it wasn’t, I’d gotten
attached to the name for my project over many months, and wasn’t up for
changing it.

[github.com/phodal/2md]: https://github.com/phodal/2md
[github.com/zshipu/2md]: https://github.com/zshipu/2md
[phodal.github.io/2md/]: https://phodal.github.io/2md/
[npm]: https://www.npmjs.com

Also someone had asked for almost exactly what I built in a [2013 Stack
Overflow question][], so it’s definitely not an original idea.

[2013 Stack Overflow question]: https://stackoverflow.com/q/16953581/14558

## Overview

The general bodies of theory drawn from here are programming language
compilation, data structures, and graph theory. Mostly it’s about the
graphs of data structures typical of the middle stages inside a compiler.
But there’s a lot of compiler theory that’s not really relevant for this
project: we’re not implementing any lexing ourselves, and there’s no
register allocation or anything.

To help make the problem manageable, we’re going to rely on two
complementary principles:

#### <small>Principle 1:</small><br/> Instead of working with strings, work with objects

Why not go directly from HTML to markdown? You might be thinking regular
expressions would be a good start, turning `<b>…</b>` into `**…**` and so
on. And indeed a sufficiently motivated person could get surprisingly far
doing that. But things will get complicated quickly, with lots of corner
cases, and you’ll end up with too much going on at once.

An example of this sort of an approach: in some HTML, to update the CSS
class on some div, it’s definitely possible to search for the markup `<div
… class="…">` and edit the contents. You might need to create the `class`
attribute first if it doesn’t already exist, and you’ll probably want to
handle variations like `class="foo"` vs. `class='foo'` vs. `class=foo`, but
it’s doable.

Although that’s doable, the code `div.classList.toggle('expanded')` is much
shorter, easier to read and understand, faster to run, and less
error-prone.

There’s some cost of parsing the HTML into a data structure and turning it
back into a string at the end, but when there are many operations to
perform, or a desire to handle variations and corner cases, or when you
start to think about the problem of specifying *which* div to operate on;
the increased simplicity and precision of working with objects to perform
all those operations add up to greatly outweigh the cost of parsing the
input into some

By working at this higher level of abstraction we gain precise focus,
much more readable code, and a chance at clean APIs.

In fact the Document Object Model standard aka DOM is a fine data structure
for HTML input, and that’s what 2md uses for its input stage. It’s the
default API in browser environments where you also get a free parser by
setting `innerHTML` on elements. `jsdom` provides command-line tools with
the same DOM API as in the browser. And whatever issues the DOM API may
have for HTML, those are made up for many times over by its ubiquity and
familiarity.

#### <small>Principle 2:</small><br/> Transform the input through a sequence of data types

To turn HTML into markdown, we’re going to perform a series of
transformation steps to make it a bit more markdown-like until it
eventually *is* markdown.

<p>
<img className="center-md" src="/doc/flow.svg"/>
</p>

In this transformation sequence, sometimes the output of a step will be the
same as its input type—that’s what the loop is indicating—and sometimes
it’ll be a new output type more suited to future steps.

Though there are many benefits to using the DOM API to represent HTML, it’s
a poor fit for representing markdown. It has a bunch of stuff we don’t
need, like `classList`, and it doesn’t include a bunch of stuff that we do
care about for markdown, like the index of an `<li>` inside an `<ol>`.

So we simplify by introducing a new data structure called an intermediate
representation: something that’s logically between two other
representations, and has similarities with both but isn’t actually either.

For 2md, the intermediate representation is called `Ir`, and it’s not HTML
anymore, though it’s close, and it’s not quite markdown yet, though it’s
also close. I could have come up with a more descriptive name than
Ir—markml? htdown? mdish?—but calling it an IR, or IL for ‘intermediate
language,’ is traditional. Sometimes the term ‘intermediate representation’
gets very confusing if you end up with a bunch of different intermediate
representations inside a compiler, with all of those different
representations having very similar names. For 2md, there is another
intermediate representation called OutputBlocks, but the names are
different enough to avoid confusion.

How many different intermediate representations do you need? Well, it
depends. I started out with two, DOM and Ir, and later added a third called
OutputBlocks when that seemed prudent. It’s something you learn about the
problem space as you write code that works in it. If the current
representation is getting too complicated, or you’re adding a bunch of
specialized functionality, it makes sense to introduce a new
representation. And sometimes you end up with too many intermediate steps
that are too similar that just waste a lot of time passing the same data
around, and waste your time when you’re trying to find the part where
things actually happen, and you edit the code to reduce the number of
intermediate representations.

It’s a lot like object-oriented programming, where you often start out with
one class, and then that one class starts doing too many different things,
split it up into different more-focused classes. Except here instead of a
single class, each representation is composed of a whole family of classes.

Incidentally, even if you weren’t working in a vacuum and reused an
existing library like `mdast` to represent the markdown output, I expect
that the code would be easier to write and maintain with an intermediate
representation expect between the DOM and markdown representations.

## Details

All right, finally it’s time to look at some source code excerpts, and some
real live data structures!

Here’s that selector/paste box again:

<DemoContent/>

To see all the data structures side by side, visit the <a href="/demo">demo
page</a>.

### Raw HTML

The raw HTML for these snippets is generally pretty messy:

<DemoHtmlEditor/>

These examples mostly come from copying and pasting from Safari on a mac.
Other browsers seem to be similar, but the important thing is that to make
note of is that, in order to make the snippet self-contained, the browser
took all the computed styles from all the CSS and inlined it on every
element, making the markup really messy. And glomming everything onto one
line makes it a lot harder to read.

Instead of staring at a <DemoCharCount/>-character string, let’s pop it
into a DOM tree to collapse the `style="…"` nodes and reveal the tree
structure.

### DOM tree

Parsing HTML into DOM objects is a very important part of the browser’s
reason for existence, and it’ll just do that naturally if formatted text is
pasted into a `contentEditable` area, at which point your object reference
to the `contentEditable` is your handle into the DOM tree.

To get DOM objects from HTML *strings* inside a browser, you can create a
new document and then set the body HTML:

    const doc = document.implementation.createHTMLDocument();
    doc.body.innerHTML = 'Hello, <b>world</b>';
    doc.documentElement.outerHTML;
    // returns "<html><head></head><body>Hello, <b>world</b></body></html>"

In node.js, the `jsdom` package provides a string-taking constructor
directly, and returns objects with the same API that you’d get in the
browser:

    const { JSDOM } = require('jsdom');
    const doc = new JSDOM('Hello, <b>world</b>').window.document;
    doc.documentElement.outerHTML;
    // returns "<html><head></head><body>Hello, <b>world</b></body></html>"

At this point, you’ve got DOM `Node` objects with familiar APIs like
`node.childNodes`, `node.appendChild()`, `node.replaceChild()`,
`node.nodeType` which has to be compared against various integer constants;
essentially all the stuff that jQuery does behind the scenes.

<DemoDom/>

### Building the intermediate representation

How to start turning that DOM tree into markdown?

Well, there’s lots that can be thrown away, like any HTML comments, the contents
of any `class=""` attributes; and we don’t care about `<div>`s or `<span>`s; …
in fact we’re going to throw away everything except for text and a small number
of tags, and do some normalization such as mapping both `<i>foo</i>` and
`<em>foo</em>` into the same thing because they’ll both come out the markdown
end as `*foo*`.

And this is where Ir comes into play: it’s a simplified, normalized data
structure to make the HTML input more markdown-like.

Each Ir node has:

  - A name, and possibly an attribute, from the following list, roughly
    corresponding to the subset of HTML that’s relevant to generating
    markdown:
      - A (attribute: href)
      - Blockquote
      - Bold
      - Br
      - Code
      - Heading (attribute: level)
      - I
      - ListItem
      - NumberedListItem (attribute: number)
      - OrderedList
      - P
      - Preformatted
      - Document
  - A list of children, all of which are either other Ir nodes or plain
    strings.

The code to actually do this is in `src/parse.ts`:

  - If the current DOM node is text, add it to the current Ir node’s
    children
  - If the current DOM node is an ‘interesting’ HTML tag, create a new Ir node
    for it, and add it as a child of the current Ir node
  - Then recurse down into any child nodes of the current DOM node

The code:

    function parse(doc: Node): IrNode {
      const root = new Document([]);

      for (let i = 0; i < doc.childNodes.length; i++) {
        parse1(root, doc.childNodes[i]);
      }
    }

    // This is the actual recursive function; I think I picked up the naming
    // convention from Lisp. When a recursive function needs accumulators, `foo()`
    // is the simplified API used by callers that does the initial setup, and
    // `foo1()` recurses.
    function parse1(irNode: IrNode, htmlNode: Node) {
      if (
        htmlNode.nodeType == htmlNode.TEXT_NODE &&
        htmlNode.textContent !== null
      ) {
        if (
          htmlNode.textContent !== "\n" ||
          irNode.isOrHasParentNamed("Preformatted")
        ) {
          irNode.push(htmlNode.textContent);
        }
      } else if (htmlNode.nodeType == htmlNode.ELEMENT_NODE) {
        const e = <Element>htmlNode;
        let receiver = irNode;
        switch (htmlNode.nodeName) {
          case "H1":
          case "H2":
          case "H3":
          case "H4":
          case "H5":
          case "H6":
            receiver = new Heading([], {
              level: extractHeadingLevel(htmlNode.nodeName)
            });
            break;
          case "B":
          case "STRONG":
            receiver = new Bold([]);
            break;
          case "I":
          case "EM":
            receiver = new I([]);
            break;
          case "OL":
            receiver = new OrderedList([]);
            break;
          case "LI":
            receiver = new ListItem([]);
            break;
          case "A":
            const href = e.getAttribute("href");
            if (href) {
              receiver = new A([], { href });
            }
            break;
          case "PRE":
            receiver = new Preformatted([]);
            break;
          case "P":
          // There’s no standard markdown representation for these, but putting them
          // on separate lines is better than jamming them together in a single
          // paragraph.
          case "DL":
          case "DT":
            receiver = new P([]);
            break;
          case "BLOCKQUOTE":
            receiver = new Blockquote([]);
            break;
          case "TT":
          case "CODE":
            receiver = new Code([]);
            break;
          case "BR":
            receiver = new Br([]);
            break;
        }
        if (irNode !== receiver) irNode.push(receiver);

        // Recurse
        for (let i = 0; i < htmlNode.childNodes.length; i++) {
          const c = htmlNode.childNodes[i];
          parse1(receiver, c);
        }
      }
    }

The result is a tree, a lot like the DOM tree, but with only about a dozen
possible node types, only three of which can have any attributes.

<DemoUntransformedIntermediate/>

### Tree transforms

Now that we’re getting closer to markdown, we can perform some operations
directly on the Ir tree, without moving to a new representation.

Here are some examples of tree transform functions from
`src/tree-transforms.ts`.

  - Turn things that started out as `<pre><code>…</code></pre>` into plain
    Preformatted Ir nodes, without an inner Code node.

    Markdown has two different kinds of code blocks: *inline
    code*, <code>&#96;foo&#96;</code>, normally rendered as <code>&lt;code
    …/&gt;</code>, and *code blocks*, typically designated with a
    four-space-indent inside markdown source

        function foo () {
            ...
        }

    that gets rendered into HTML as `<pre><code>…</code></pre>`.

    When going from HTML to markdown, `<code>` should turn
    into <code>&#96;foo&#96;</code>, and 2md has a Code class to represent
    inline code. Meanwhile HTML of the form `<pre><code>…</code></pre>`
    should turn into a Preformatted Ir node, with no Code object,
    to represent a code *block*.

    The initial Ir tree comes from a fairly direct mapping of HTML tags, so
    also ends up with a Code node as a child of a Preformatted node.
    There’s no need for that, and we don’t want to emit four-space-indented
    blocks with an extra <code>&DiacriticalGrave;</code> in there,
    so that should be simplified.

    The code to do that is

        function collapseCodeInsidePre(node: IrNode) {
          if (node.name !== "Preformatted") {
            return;
          }

          if (node.childCount() === 1) {
            const c = node.child(0);
            if (typeof c !== "string" && c.name === "Code") {
              node.replaceChildWithItsChildren();
            }
          }
        }

    which is called from a wrapper function that handles the traversal
    boilerplate.

  - For `<ul><li>…` markup we can emit `"  - "` markdown without regard for the
    context surround where the `<li>` occurs.

    But for an `<li>` inside an `<ol>`, we need to emit a number
    corresponding to the index in the parent list.

    So there’s a `numberLists` transform which walks along, counting, and
    sets the correct `index` attribute on every NumberedListItem.

  - Concatenate adjacent string children into a single string, purely to
    make it easier to understand what is going on. This is especially
    noticeable for the syntax highlighting in the ‘Node JS’ and ‘Stack
    Overflow’ examples, where each `<span … />` for syntax highlighting
    ends up being a separate string without this optimization.

        /**
         * If two subsequent nodes are strings, concatenate them. This optimization pass
         * should be quick, and is here entirely to make unit test input cleaner.
         */
        function concatenateStrings(node: IrNode) {
          for (let i = 0; i < node.childCount() - 1; i++) {
            const c0 = node.child(i);
            const c1 = node.child(i + 1);
            if (typeof c0 === "string" && typeof c1 === "string") {
              node.setChild(i, c0 + c1);
              node.removeChild(i + 1);
              // Since we just deleted the element at the current index, decrement
              // the loop index, otherwise we’ll miss the next element which has
              // shifted down into the current position.
              i--;
            }
          }
        }

With all that and more, the output is the same Ir tree as before, now a bit
more markdown-ish.

<DemoIntermediate/>

We’re almost ready for output!

### OutputBlocks

There’s one final important thing that Ir objects have: a `render()`
method. This is how the Ir tree gets turned into something much closer to
markdown.

All the Ir classes are defined in `src/2md.ts`, and here’s one of them:

    export class Code extends IrNode {
      static irName: NodeName = "Code";

      render(r: BlockRendering) {
        r.append("`");
        super.render(r);
        r.append("`");
      }
    }

Pretty easy! To output an inline code block: output a <code>&#96;</code>
character, call the superclass `render()` method to render any children of
this node, and then output another <code>&#96;</code> character.

That example uses inline markdown. What about a block?

    export class ListItem extends IrNode {
      static irName: NodeName = "ListItem";

      render(r: BlockRendering) {
        const prefix = new Prefix("  - ", "    ");
        r.pushPrefix(prefix);
        super.render(r);
        r.popPrefix(prefix);
      }
    }

Pretty straightforward: add a Prefix of `"  - "` for the first line of the
bullet point and `"    "` for subsequent lines, recursively render the
`IrNode`’s contents, then stop using that Prefix since the block is done.

The interface for `BlockRendering` is pretty small:

    export declare class BlockRendering {
      append(s: string, newline?: boolean): void;
      pushPrefix(prefix: Prefix, wrapOptions?: BlockOptions): void;
      popPrefix(prefix: Prefix): void;
      /** Used for collecting link info to add on at the end */
      addTrailer(s: string): void;

      finish(): string;
    }

At this point, it may look like it’s possible to write out the final
markdown string, but my attempts at that were problematic. Sure, a lot of
html can be correctly handled that way, but there is an issue that requires
more structure to handle.

Nesting. Nesting gets complicated.

For example, if there’s a code block in a bulleted list inside a
blockquote, the start of the line is

    >   -     foo

which renders back to HTML as

>   -     foo

but if that same code block starts on a later line of the bullet point,
there’s no bullet-point marker on the same line as the start of the code
block:

    >   - Here is some code:
    >
    >         foo
    >
    > …

and that renders back to HTML as

>   - Here is some code:
>
>         foo
>
> …

The render function inside the Code class doesn’t and shouldn’t know
whether it’s on the first line of a bullet point item or not.

After trying out a number of unsuccessful hacks, I decided that the issue
was complicated enough to justify introducing a new intermediate
representation called an OutputBlock, roughly corresponding to a paragraph.

Instead of `IrNode.render()` appending to the final markdown string, it
writes into an OutputBlock object. A new OutputBlock automatically gets
created whenever the Prefix changes.

Each OutputBlock contains two key things:

  - block content in the form of a string that’s already markdown

  - a stack of Prefix objects tracking what needs to be inserted at the
    beginning of each line

The Prefix class is fairly simple: the text to use on the first line, the
text to use on subsequent lines, and whether or not it’s already been
rendered.

    /**
     * A string that gets prepended to each line of an output block in Markdown,
     * e.g., `#` or `>`. The first line is often different from subsequent lines,
     * for example with a bulleted list that is simply indented on subsequent lines.
     *
     * You should always make a new prefix instead of caching one, because it
     * contains state to track whether the first line has been printed or not.
     */
    export class Prefix {
      readonly first: string;
      readonly subsequent: string;
      private _rendered = false;

      constructor(first: string, subsequent?: string) {
        this.first = first;
        this.subsequent = subsequent !== undefined ? subsequent : first;
      }

      equals(other: Prefix): boolean {
        return this.first === other.first && this.subsequent === other.subsequent;
      }

      static render(prefixStack: Prefix[]): string {
        let ret = "";
        for (let p of prefixStack) {
          if (!p._rendered) {
            p._rendered = true;
            ret += p.first;
          } else {
            ret += p.subsequent;
          }
        }
        return ret;
      }
    }

With this structure, we can be sure that when a block, for example a bullet
point, requires a different line-start on its first line, that first line
is definitely used, and only once. Even if there’s a lot of deep nesting.

Here’s that OutputBlock representation, which is much much closer to
finally being markdown:

<DemoOutputBlocks/>

The structure here is usually pretty boring because there’s usually not a
lot of nesting in typical documents, but is particularly noticeable on the
‘Nested lists’ example.

Any `""` prefix blocks are used as paragraph markers; empty lines will
later be emitted at the end of every output block.

By the way, any links here haven’t been lost; they’re stored in the object
that holds the `OutputBlocks[]` list, and will be appended at the end of
the final markdown text.

### Markdown

After wrapping the text in each OutputBlock, and prepending the prefix to
each line—finally we have markdown!

<DemoMarkdown/>

</DemoFlow>

<div className="clearfix"/>

## The development process

2md, and the details I’ve given about how it works, didn’t pop into my
head (or out of it onto my computer) fully formed. I’m sharing the
development process here in the hopes that it might help you get started on
something too.

To start, I chose one clipping, the Ars Technica excerpt above, by
scrolling through the last article I’d manually clipped something out of,
looking for a small piece with a good variety of formatting. It has a
heading, bold text, links, bullet points, and nothing else. Something
simple to start.

Then I worked on the Ir stuff to pass a single unit test to turn that
document into Ir nodes:

    it("can parse the first sample", async function() {
      const html = await fixture("quote1.html");
      const expected = new Document([
        new Heading(["The end of 32-bit apps (and other removals)"], {
          level: 2
        }),

        new P([
          "Mac hardware and macOS made the jump from 32 bits to 64 bits a long" +
          ...

with the expectation that those Ir nodes could then append their contents
to a string. But I didn’t try to do anything on that front until that first
unit test passed. Then I added rendering of Ir into a string, including
text wrapping.

Once that worked end-to-end, again with just headings, bold, links, and
bullet points, I knew I was on the right track.

At this point I took some time to add a readme, commented the code a bit,
wrote a launcher script, and pushed everything to GitHub. I poked a few
people who might be interested in working on it with me, and though nobody
was interested enough to start writing code, I did receive encouragement.

I’d been trying to use the code while working on it, and that was clearly
showing me where it needed work. For example, the MDN excerpt got into the
test suite because, when looking up how to do some basic thing with regular
expressions, 2md did a terrible job of pasting what I found into my notes.

Every time I used it, I’d notice things it needed, and worked down the list
adding support for them until they were pretty much all taken care of. Some
were easy, like adding italics support by copying the code for bold and
changing a handful of characters; others, like figuring out how to handle
nested lists, were much more work.

As far as testing goes, one big advantage here over typical
compiler-writing is that there are a lot of existing tools to turn markdown
into HTML, so it’s easy to create end-to-end test cases. I could write test
cases in markdown, have the test convert that to HTML using some existing
library, and verify that 2md would turn that HTML back into the original
markdown. There’s certainly no tool for C compiler writers that turns
binaries back into source code in a way that validates the correctness of
the compiler’s code generation.

Incidentally, that means that my knowledge here might be obsolete in the
sense that this might be a very good problem for machine learning:
existing markdown-to-html libraries can generate vast amounts of training
data for learning a model to go in the opposite direction.

In this case, I did things the old-fashioned way, looking at text dumps of
data structures on the terminal to figure out where things were going
wrong, and inventing new data structures and algorithms to make things go
right. 2md has a `--output-format` flag for debugging; it dumps text
versions of the structures in this post. I didn’t have interactive
in-browser data structures at the start, but now that I’ve learned how, I’d
likely try to get those in from the beginning.

### JavaScript

*[The next few subsections are more opinion-based, skip ahead to <a
href="#complexities">the next section</a> if you’d like to stick to the
technical bits.]*

This project is written in JavaScript and I suggest that it should be the
default language for most projects for most people. For actual webapps, of
course, which is hardly a controversial opinion; but also scripting,
automation, little throwaway command-line utilities: do it all in
JavaScript.

Why?

To start, there are some convenient things about working with node.

The node.js project provides static relocatable binaries for almost every
platform, including Raspberry Pi, so it’s fast and easy to get any
supported version of node running on almost any machine, even if you don’t
have root access. It’s just `wget https://… && tar …`, even for the very
latest versions.

And those distributions of node include `npx` which makes it really easy to
run tools off of npm; `npx 2md` should just work anywhere. Need any tool at
all on some random machine you’re SSHed into, like maybe a curses json file
explorer? `npm fx`. Write your tools in JavaScript and it can be that easy
for others to use your software too.

But outside the command-line world there is the browser. Which is how most
people use software.

If your goal is to build software that other people actually use, you might
as well make something where people can click a link and immediately start
using it. If your users would benefit in some way from interactivity or
visuals or networking, use JavaScript and make it a webpage. If you want to
have interactivity or visuals, even for inspecting the internals of your
program, or if you might want somebody to be able to remotely perform an
action using your software some day, write it in JavaScript to begin with
so that you can later add those benefits without having to rewrite
everything in a new programming language.

I built 2md primarily as a command-line program with the intent of later
enabling it to work in a browser. Doing so was pretty anti-climactic. I had
to tidy up some dependency references so it wasn’t trying to touch the
filesystem, but that was pretty much it.

### TypeScript

TypeScript is a popular variant of JavaScript that adds type annotations to
JavaScript. 2md is written in TypeScript, and I found it really useful for
its linting capabilities.

I think I can explain, in fairly simple terms, why using types appeals to
me so much. I make mistakes when I’m coding. It’s unavoidable. We’re taking
vague, uncertain ideas and trying to turn them into precise instructions
for unforgiving machines. I believe something is precise, but not until I
try to make the code work do I find out that it is not, and I will need to
think about it more to get it right.

There’s a whole class of low-level bug-inducing detail that is really easy
for machines to catch and really easy for humans to miss. Let’s let them
help us by letting them do that.

  - “‘BlockQuote’ is not a valid option here, but ‘Blockquote’ is.”

  - “You are trying to give a number to a function that says it only
    accepts strings.”

  - “What would you like to have happen if `undefined` is passed to this
    function—surely you do not wish the program to crash in the event of
    such an occurrence?”

  - “If you add an argument to this function signature, as you are surely
    already aware, you are going to have to update these twelve callers of
    the function. I have highlighted them for you.”

Thank you, computer. That was very helpful.

Using TypeScript helped give me confidence when refactoring. The prefix
parameter started out as just a string, then was a pair of strings,
eventually becoming its own object that tracks if it’s been rendered yet
and is attached to a new OutputBlock structure. And through every step of
that process, the type-checker showed me all the areas that were affected
by those changes so I could update them appropriately, instead of being
surprised by an easily-detectable error.

There are definitely some drawbacks to TypeScript. It is tricky to learn,
and if someone is just starting out with JavaScript I would actively
discourage working in TypeScript at the start. That way you can learn the
fun of working in JavaScript without risking long fights with corner cases
of the type system. There’s only so much you can learn at once, and better
to have some buggy code that mostly works, was fun to write, and can be
cleaned up by adding type-checking later, than a perfectly-typed program
that never grew big enough to be useful.

The TypeScript compiler is intended to do type-checking and then emit
JavaScript code, but I’ve found using babel to be a much more pleasant
experience for the code-generation part. `@babel/preset-typescript`
essentially throws away all the type annotations—a very quick
process!—leaving you with plain JavaScript to flow through the rest of the
babel pipeline. With `@babel/register`, that happens at runtime, allowing
you to to run `.ts` files directly, without waiting for the typescript
compiler, and even if they have type errors because you’re experimenting.

And the 800-lb gorilla in the room is microsoft, the corporate sponsor of
TypeScript. As the topic of microsoft’s history, present, and possible
futures is something much bigger than I am able to go into right now, I
will keep things relatively short. While I find TypeScript highly useful, I
expect microsoft’s primary overriding long-term motivation to always be
economic, and that does not align with indefinitely spending lots of money
giving away valuable things and not somehow getting something more valuable
in return.

Please don’t interpret my use of TypeScript (or react or
bootstrap) as an endorsement of any of microsoft’s (or facebook’s or
twitter’s) products, services, business practices, or impacts on society.

Should the powers-that-be at microsoft some day decide that TypeScript no
longer makes financial sense to continue to invest in, at least we will
have the option of dropping all the types from our code and going on with
JavaScript alone. Type systems for JavaScript and dialects of JavaScript
will come and go, but JavaScript itself will remain highly relevant for a
very long time.

## Complexities

The earlier technical stuff was a reasonably straightforward application of
existing theory around parsing and transforming structured text.

Now we get more into what Fred Brooks Jr. termed accidental difficulties:
all the things we have to do to get the problem solved, that aren’t part of
any abstract platonic solution, but are consequences of the machines,
tools, and conventions that we happen to be using.

### Line wrapping graphemes

The markdown I work with is text hard-wrapped at a very traditional 80
columns. To get that in 2md’s output, I wrote some line-wrapping code. Each
OutputBlock contains a stack of Prefix objects and some markdown text that
hasn’t been wrapped yet, so the algorithm measures the width of the prefix,
subtracts that from the 80-column allowance, and wraps the text to fit in
what remains.

Line-wrapping can be a very involved topic; the <span
class="tex">T<sub>e</sub>X</span>book goes into it in considerable detail.
With proportionate fonts and proportionate spacing, justified text, and
hyphenation, you want to consider many possible different break points
between words. Then for all the possible sets of breakpoints, a ‘niceness’
function is applied to the aesthetics of the resulting lines, and the set
of breakpoints that maximizes niceness is chosen. Dynamic programming is
used to keep runtime low in the face of possible combinatorial explosion of
subcases.

An example of where this complexity is warranted: if one line ends with a
short word like ‘a’ or ‘be’ or ‘the’, often the overall alignment can be
made much more even by moving that word, even if it’ll fit, to the start of
the next line.

As an example from [“Breaking Paragraphs into Lines”][knuth-plass], the
greedy algorithm which tries to fit as much as possible into each line
produces:

[knuth-plass]: https://dx.doi.org/10.1002/spe.4380111102

<div class="non-breaking-quote">
has seen so much, was astonished whenever it shone <b>in</b><br/>
her face. Close by the king’s castle lay a great dark<br/>
forest, …
</div>

But if that ‘in’ is moved to the next line, which has enough room for ‘in’
but not for ‘forest’, the line lengths are much more balanced:

<div class="non-breaking-quote">
has seen so much, was astonished whenever it shone<br/>
<b>in</b> her face. Close by the king’s castle lay a great dark<br/>
forest, …
</div>

I was able to convince myself that for monospaced text, the greedy
algorithm will give an acceptable appearance for now.

However, one area of complexity here is around Unicode. The `length`
property of a JavaScript string reports the number of UTF-16 code units,
which is not what we’d normally think of as character length.

There are two main issues: surrogate characters and combining characters.

First, surrogate characters. JavaScript strings are made up of 16-bit
characters, allowing at most 65,536 possible character values.

[After JavaScript was invented][unicode-history], in 1996 Unicode was
expanded to allow many more possible character values, and a standard
encoding mechanism was defined for representing character values that need
more than 16 bits: a pair of 16-bit surrogate values. These 16-bit values
fit into a JavaScript character but only represent a Unicode character when
combined with another complementary surrogate.

[unicode-history]: https://learning.oreilly.com/library/view/unicode-demystified/0201700522/0201700522_ch02lev1sec5.html

Consider ‘𝒾’, U+01D4BE “Mathematical script small i”, Unicode character
number #119,998 in decimal. Its code point does not fit into 16 bits, so
JavaScript thinks it’s a 2-character string, composed of the surrogates
0xD835 and 0xDCBE:

    > i = '𝒾'
    '𝒾'
    > i.length
    2
    > i.charCodeAt(0).toString(16)
    'd835'
    > i.charCodeAt(1).toString(16)
    'dcbe'

Line-wrapping should account for that when measuring the width of words.

The second issue has to do with combining characters.

Many characters are variations on other characters. For example ‘ü’ U+00FC
“Latin small letter u with diaeresis” is a ‘u’ with umlauts added on top.
Many characters like this have their own code point for historical
compatibility reasons, but even Unicode doesn’t have room to predefine
every possible variation of every character as a separate code point.

To allow *arbitrary* variations on characters, the Unicode standard defines
combining characters: the same ‘ü’ character may also be represented as
plain ‘u’ U+0075 followed by U+0308 ‘ ̈’ “Combining Diaeresis”. And this
mechanism lets you create characters like ‘j̈’ by adding combining
characters wherever you want.

As an extreme case, this is sometimes taken to a form of art

‘[`H̸̡̪̯ͨ͊̽̅̾̎Ȩ̬̩̾͛ͪ̈́̀́͘ ̶̧̨̱̹̭̯ͧ̾ͬC̷̙̲̝͖ͭ̏ͥͮ͟Oͮ͏̮̪̝͍M̲̖͊̒ͪͩͬ̚̚͜Ȇ̴̟̟͙̞ͩ͌͝S̨̥̫͎̭ͯ̿̔̀ͅ`][bobince]’

[bobince]: https://stackoverflow.com/a/1732454/14558

which at 7 letters and one space easily fits onto one line, but with all
the combining characters ends up at 83 code units long, and would be broken
into two by a naïve line breaking algorithm trying to keep things to 80
columns.

Now this is one part of 2md where I did not reinvent the wheel. Because the
term ‘character’ is so overloaded, this abstract idea of a distinguishable
single entity made out of some potentially large number of Unicode code
points, themselves made out of a potentially larger number of code points
thanks to surrogates, is termed a *grapheme*.

I just used the grapheme-splitter package to measure word length:

    const splitter = new GraphemeSplitter()
    …
    const length = splitter.countGraphemes(text.substring(…));

and it does its job well, accounting for both surrogates and combining
characters. It would be good to add a call here to `wcswidth()` to account
for the double-width of graphemes such as emoji; I am currently using the
excuse that I did not find an up-to-date JavaScript library for that.

### Command-line speed

I was initially focused on functionality, not speed, but once I had 2md in
a good state and was using it more casually, it did seem slow.

Running it on my mac takes around 1.6s in development, with
`@babel/register` configuring itself at startup, intercepting all the
compiles, throwing away types, turning `import`s into `require`s, and other
transpilation tasks. This is still much faster than waiting for the
typescript compiler to run.

But it was also taking a bit over a second with the code that’s shipped to
npm, where there is zero babel work done at runtime. A second isn’t very
long on an absolute scale, but enough to be a noticeable pause that can
break concentration.

Fortunately, in comparison to previous ancient experiences with open-source
profiling tools, I was very happily surprised by the quality of the node
profiler.

It’s as simple as running `node --cpu-prof …` to drop a `CPU.….cpuprofile`
JSON file in the current directory, and then in Chrome Devtools, on the
performance tab, clicking the ‘Load Profile’ button to open that JSON file.

<DemoImage src="/doc/load-profile.png"/>

This yields an interactive flamegraph:

<DemoImage src="/doc/lib-overview-flamegraph.png"/>

There are three main phases here:

  - Loading libraries
  - Waiting for a shell command to return clipboard contents: the blank
    space in the middle
  - Actual html-to-markdown work

and loading libraries is dominating.

Clicking on any of the items shows details of the function call at the
bottom of the ‘Summary’ tab. During the library-loading phase, there are a
lot of calls to `Module._compile` which has an `(anonymous)` child which
corresponds to running the top-level module code of imports. And here we
see that simply loading `jsdom` is, by itself, taking more than half the
total runtime.

My first thought on seeing long load times is to try bundling. Maybe it’s
just taking forever scanning the filesystem, repeatedly checking hundreds
of different folders for hundreds of little javascript files that together
make up the library. Maybe the bundler will infer that some import will
never ever be used so it’ll skip loading it entirely.

It took a fair bit of fiddling to get the bundler to run without crashing.
Key to this were a few edits to `package.json`.

First,

    "browserslist": "current node",

tells parcel that it should generate code to target the version of node
under which it is running, using all the latest language features instead
of trying to generate code that is backwards-compatible with old browsers,
or adding dependencies on needless polyfills that would then need to be
imported.

Second, jsdom will look for the `canvas` library, and load it if it exists,
so that jsdom users can actually draw to `<canvas>` objects in their
programs or unit tests and save the outputs to files—pretty neat, but not
of use to generating markdown. And parcel was confused both if the canvas
library wasn’t installed, in which case it thought there was a missing
dependency, or it would fail to bundle the canvas module if it *was*
installed; problems that were rectified by adding

    "alias": {
      "canvas": "./src/canvas-shim.js"
    },

where `src/canvas-shim.js` contains `module.exports = {};`

Also parcel does not like `#!/usr/bin/env node` in any files it imports;
it’ll bundle the files but then node will crash on seeing that in the
middle of a bundled JS source file.

And `require.main === module` checks do not work in parcel—once bundled,
both everything and nothing is the main module in some sense—so if you’re
using that in your main launcher script so that you can also import it from
unit tests, you’ll need a separate parcel launcher script without that
guard.

But after all that, following invocation worked

    parcel build --bundle-node-modules --target=node -o 2md.js src/parcel-main.ts

and then it’s just running node on the resultant standalone bundled file:

    node 2md.js

With that improvement, runtime is down to 0.6s which at least to me feels
instant: I don’t start thinking about other things while waiting for it to
finish. I’m not sure how much tree-shaking, if any, is going on here, but
I’m happy with the result.

<DemoImage src="/doc/bundle-overview-flamegraph.png"/>

However I’m not sure about the etiquette here of distributing a module with
a bundled copy of all its dependencies in a single 3MB file. So if you’re
using the npm version you’re getting the slower unbundled version for now.

Well, most of the time seems to be spent in the accidental parts of the
program. What about the essential bits? Zooming in on the actual
translation work that’s done:

<DemoImage src="/doc/operation-flamegraph.png"/>

It looks like the split is roughly half the time is spent in JSDOM parsing
the HTML the other half of the time spent running GraphemeSplitter, and
roughly zero time spent on html-to-markdown conversion. Taking so little
time makes sense for a handful of paragraphs of markdown; that may look
very very different on large inputs though.

Only in hindsight do I realize that jsdom is a pretty heavyweight library;
I use it frequently for the huge convenience of having the same API as the
browser, but it’s much more than simply an HTML parser + `Node`/`Element`
implementation.

jsdom does bill itself is as much more than a DOM API:

> jsdom is a pure-JavaScript implementation of many web standards, notably
> the WHATWG DOM and HTML Standards

For testing purposes, it can act as a headless browser, including script
evaluation and triggering click handlers and so on, which is fantastic for
testing and presumably much faster than firing up a real headless browser,
but not so much if you don’t want to wait a half-second for it to load just
to parse a few kB of HTML. On another project I’ve used `parse5`, the
underlying HTML5 parser used by `jsdom`, and found it to be much faster to
run, but much slower to write code for as it has its own API. I may switch
to that eventually.

### Reading from the clipboard

I was hoping there’d be some nice cross-platform way to read HTML from the
clipboard. My web searches show that there is in Electron, because it is
linked against Chromium which implements HTML copy-paste in C++ across all
platforms; but not in node.js.

On the mac it turned out to be [straightforward to run some
AppleScript][mac-clipboard]:

[mac-clipboard]: https://stackoverflow.com/q/17217450/14558

    $ osascript -e 'the clipboard as «class HTML»'
    «data HTML3C7370616E207374796C653D22…223E68656C6C6F20776F726C643C2F7370616E3E»

After unhexlifying that, it’s plain old HTML:

    <span style="">hello world</span>

This would likely run faster if there was some native code to read the
clipboard directly, but figuring out how to do that with node really didn’t
seem like a worthwhile use of my time at the beginning of the project.

And on linux, assuming `xclip` is installed, again it’s fairly easy
to shell out:

    xclip -o -selection clipboard -t text/html

and again, native code has potential speed advantages but adds complexity.

But windows was trickier.

#### Firewalling windows

I’d like my software to work for people who are forced to use windows, but
with things like ads for candy crush in the start menu, every program or
document search I type into the start menu being sent to microsoft with no
opt-out, [microsoft silently disabling the ability to use local user
accounts][force-ms-acct], and that search-query-history plus forced-login
combination used to build personality profiles which are sold to
advertisers—at this point I have to treat windows itself as malware. And I
will continue to do so until microsoft adds a clear toggle to disable
sending any data back to microsoft.

[force-ms-acct]: https://www.howtogeek.com/442609/confirmed-windows-10-setup-now-prevents-local-account-creation/

Fortunately there are techniques for dealing with malware. I used a
proxy-only network.

 1. In VMware Fusion → Preferences → Network, create a new network and
    disable all the options around connectivity to your mac, external
    networks, or getting DHCP addresses. Double-click the network’s name to
    rename it if you want to better be able to keep track of different
    virtual networks here.

    <DemoImage src="/doc/vmware-network-settings.png"/>

 2. Create a linux VM, and give it two network adapters: the first on any
    network with external access, the second on your proxy-only network.

 3. Pick an IP range for the proxy-only network; I used 192.168.237.0/24.
    Configure your Linux VM to do normal DHCP on the main interface,
    and to have a static IP of .1 on the proxy-only interface.

 4. Start a DHCP server on the linux VM. All it’ll do is give out IP
    addresses, so `/etc/dhcp/dhcpd.conf` can be as simple as

        subnet 192.168.237.0 netmask 255.255.255.0 {
            range 192.168.237.10 192.168.237.100;
        }

    There’s no need to specify a router or nameserver because there won’t
    be any on this network.

 5. Install squid on the linux VM, and edit `squid.conf` so that all the
    default `allow` lines are gone, and configure a limited number of
    domains which will be allowed through the proxy, with everything else
    blocked.

        --- squid.conf.default
        +++ squid.conf
        # Example rule allowing access from your local networks.
        # Adapt localnet in the ACL section to list your (internal) IP networks
        # from where browsing should be allowed
        -http_access allow localnet
        -http_access allow localhost

        +acl allowlist dstdomain .debian.org
        +acl allowlist dstdomain .nodejs.org
        +acl allowlist dstdomain .npmjs.org
        +acl allowlist dstdomain .yarnpkg.com
        +
        +# Windows update only
        +acl allowlist dstdomain .update.microsoft.com
        +acl allowlist dstdomain .download.windowsupdate.com
        +acl allowlist dstdomain .delivery.mp.microsoft.com
        +acl allowlist dstdomain .ctldl.windowsupdate.com
        +
        +http_access allow allowlist
        +
        # And finally deny all other access to this proxy
        http_access deny all

    The update domains here are a subset of those [documented by
    microsoft][ms-update]; only enough to get windows update running for
    me.

    [ms-update]: https://docs.microsoft.com/en-us/windows-server/administration/windows-server-update-services/deploy/2-configure-wsus#211-connection-from-the-wsus-server-to-the-internet

 6. Create another linux VM with a single network adapter on the proxy-only
    network and do some testing. Make sure you can’t ping the outside
    world. Make sure you can’t do DNS lookups. Try using the proxy:

        https_proxy=http://192.168.119.139:3128 \
        http_proxy=http://192.168.119.139:3128 \
            curl …

    You should be able to access any site on a subdomain of the allowlist,
    and nothing else.

Congratulations, you have a network on which it’s safe to run windows!
Just make sure there’s only a single network adapter, and that it’s on the
proxy-only network.

When windows is installed, it won’t know about any internet access until
you tell it about the proxy in Internet Settings. Once that’s done you can
enable the proxy system-wide:

    netsh winhttp import proxy source=ie
    netsh winhttp show proxy

The above configuration worked for me to run windows update so that I could
test compatibility of my software against the latest version.

It’s satisfying to tail the squid logs and see windows relentlessly trying
to call home to www.bing.com or login.live.com every few seconds, but
getting TCP_DENIED/403 every time. If something you want isn’t working,
you’ll see the hostnames in the squid log file, and if they’re safe, you
can always add them by editing `squid.conf` and running `squid -k
reconfigure`.

With this proxy setup:

  - For npm to work, tell it about the proxy.

        npm config set proxy http://...
        npm config set https-proxy http://...

    Yarn will automatically pick up that config.

  - To get WSL working, you can use [Manually download Windows Subsystem
    for Linux distro packages][wsl-inst] as a rough guide.

    [wsl-inst]: https://docs.microsoft.com/en-us/windows/wsl/install-manual

     1. Run

            Enable-WindowsOptionalFeature -Online \
                -FeatureName Microsoft-Windows-Subsystem-Linux

        in powershell.

     2. Download the [Debian `.appx` file][deb-appx] on a different machine
        on the VM host and drag-and-drop it to the guest.

        [deb-appx]: https://aka.ms/wsl-debian-gnulinux

        It’s just a zip file, so unzip it and double-click the Debian.exe
        file to install. After it’s done installing, hit WinKey-R for run,
        type `bash`, and hit enter.

     3. Then inside the debian WSL instance, add some configuration so that
        it too knows about the proxy.

        admin@DESKTOP-YP1DX14:/etc/apt/apt.conf.d$ cat 00proxy
        Acquire::http::Proxy "http://192.168.237.1:3128";
        Acquire::https::Proxy "http://192.168.237.1:3128";

        [This guide][buster-upgrade] can help you upgrade to the latest Debian.

        [buster-upgrade]: https://xlii.io/2019/07/11/updating-your-wsl-debian-image-to-buster/

  - Firefox, unlike IE or Chrome, allows setting a custom proxy
    configuration that only applies to Firefox. You should be able to
    configure that so that it authenticates with squid, allowing
    unrestricted web browsing from inside the VM but without enabling the
    adware and spyware built into windows.

Of course this may only bide time until microsoft starts running both
updates and invasive tracking over the same hostnames, or refuses to let
you log in without proving your identity to microsoft’s servers first, but
it’s a reasonable defense against the current level of maliciousness.

#### The windows clipboard

With a safe dev instance of windows available, it’s possible to work on
getting HTML off the clipboard.

The CF_HTML text format is [documented][CF_HTML], and should be easy to get
with powershell:

    C:\Users\admin>powershell Get-Clipboard -TextFormatType Html
    Version:0.9
    StartHTML:00000131
    EndHTML:00000414
    StartFragment:00000165
    EndFragment:00000378
    SourceURL:https://nodejs.org/en/
    <html><body>
    <!--StartFragment--><p>Node.jsÂ® is a JavaScript runtime built on <a
    href="https://v8.dev/">Chrome's V8 JavaScript engine</a>.</p>
    …

[CF_HTML]: https://docs.microsoft.com/en-us/windows/win32/dataxchg/html-clipboard-format

Oh dear, the dreaded [mojibake][]: `Â`. There’s some sort of encoding issue
here. The CF_HTML docs say it’s supposed to be UTF-8, but something’s
getting messed up.

[mojibake]: https://en.wikipedia.org/wiki/Mojibake

Is it a codepage issue? Let’s try putting the terminal into UTF-8 mode:

    C:\Users\admin>chcp 65001
    Active code page: 65001

    C:\Users\admin>powershell Get-Clipboard -TextFormatType Html
    Version:0.9
    …
    <!--StartFragment--><p>Node.jsAr is a JavaScript runtime built on <a
    …

Oh dear, that’s *significantly* worse.

To avoid any terminal-level encoding issues, let’s try running `powershell
Get-Clipboard` from node, capturing the output in a pipe, and printing a
hexdump. And let’s use something complicated, like an emoji, to be sure we
get it right: “The quick brown 🦊 jumped over the lazy dogs,”
where the fox is U+1f98a.

    000000b0  6d 65 6e 74 2d 2d 3e 54  68 65 20 71 75 69 63 6b  |ment-->The quick|
    000000c0  20 62 72 6f 77 6e 20 64  59 dd 53 20 6a 75 6d 70  | brown dY.S jump|

The fox is coming out as the plain-ASCII string ‘dY.S’, which is terrible
because now we can’t distinguish an emoji fox on paste from the ascii
string ‘dY.S’.

After struggling for quite a while I hit upon: let’s try avoiding any
confusion around codepages or translations between processes by having
powershell itself write to a file:

    PS C:\Users\admin>Get-Clipboard -TextFormatType Html > out
    $ hexdump -c out
    00000000  ff fe 56 00 65 00 72 00  73 00 69 00 6f 00 6e 00  |..V.e.r.s.i.o.n.|
    ...
    000008a0  71 00 75 00 69 00 63 00  6b 00 20 00 62 00 72 00  |q.u.i.c.k. .b.r.|
    000008b0  6f 00 77 00 6e 00 20 00  f0 00 78 01 a6 00 60 01  |o.w.n. ...x...`.|
    000008c0  20 00 6a 00 75 00 6d 00  70 00 65 00 64 00 20 00  | .j.u.m.p.e.d. .|

Ok, UTF-16le encoding; this is something we can work with. The fox has
turned into four code units: `0xf0 0x178 0xa6 0x160`. What is that? Is
that the UTF-8?

I know how to type out conversion much faster in Python than node, so

    >>> '\U0001f98a'.encode('UTF-8')
    b'\xf0\x9f\xa6\x8a'

We should see the four code units:

    0xf0 0x9f 0xa6 0x8a

That’s almost right. Two of them match, but why would 0x9f be mixed up with
0x178? Or 0x8a and 0x160? Is the problem on every other byte??

After beating my head against this for a long time, [something on
stackoverflow][Markus] that had seemed preposterous at first started to
seem very sane:

[Markus]: https://stackoverflow.com/a/38067962/14558

> [link to cp1252 table]
>
> Soln: Create a translation dictionary and search and replace.

Checking the cp1252 table: U+0160 is encoded as 0x8a in cp1252. Those are
the same numbers, in the opposite direction of the garbling I’m seeing.
U+0178 is encoded as 0x9f in cp1252. Again, same numbers, in the opposite
direction.

That’s it!

Inside powershell(/win32?) what seems to be happening is:

  - Get UTF-8
  - Decode as if it was cp1252
  - Encode into UTF-16le, adding BOM
  - Then translate again into some other output encoding

So, to read HTML from the clipboard on windows:

  - Run powershell, writing to a file to avoid corruption due to additional
    encoding
  - Load the resulting file as UTF-16le
  - Encode with cp1252
  - Now decode that as UTF-8

And that’s what the code’s doing now. It may not work at all on non-English
versions of windows where the default codepage is something other than
cp1252.

<DemoImage src="/doc/2md-windows1.gif"/>

## Conclusion

This was meant to be a fun challenge that produced something useful for me,
and I’m very happy with how it’s turned out.

I’ve mapped the 2md binary to typing `,2` in vim normal mode (`map ,2
:r!2md <CR>` in `~/.vimrc`) so that pasting markdown in the terminal is
just two keystrokes and a fraction of a second, and I seem to run it on
average 3 or 4 times a day.

It’s not complete by any means; although I’ve covered the most common
formatting that I tend to copy and paste, there’s a lot of room for
improvement. When I notice something off in the markdown output when
running 2md, I save the input (`2md --output-format=raw`) to a folder for
future fixes, and do minor manual fixups to avoid shifting focus from the
task at hand to working on 2md. But if other people are interested in this
tool, I could definitely start working through those samples…

Actually, when the markdown output looks funny, often it’s because the
input HTML is a bit off, e.g., `<b>Hello </b>world` comes out as `**Hello
**world`. That’s something 2md could clean up automatically, and I might do
that some time, as well as taking a look at some parsing the styles, so
that some day `<span style="…; font-weight: bold; …">foo</span>` comes out
as `**foo**`.

This feels like the first project I’ve done that’s really, really easy for
other people to use, setup/installation-wise: just click a link and get
something interactive. For the ability to give people that, I will
definitely use more TypeScript in the future.

In case you’re curious about time: it was about a week of full-time work to
get an initial version with good coverage of markdown features, including
nested lists. It took less than a day to get it running on the web, and
that was mostly dealing with tooling. Dealing with command-line speed
issues was under a day. Windows clipboard support by itself was close to
two days, and now talk preparation / article writing is getting close to
two weeks.

Relating back to the quote from Neal Ford on automation: instead of getting
bored adding markdown formatting over and over by hand, I learned a ton
automating the process, and I’m really, really happy about that. In fact
the hard part now is resisting the urge to simultaneously explore all the
new avenues this has opened up.

— Andrew.

</div>
